---
description: 
globs: 
alwaysApply: true
---
# Best Practices for Creating Label Studio ML Backends

This document outlines guidelines for building new ML backend examples in the `label-studio-ml-backend` repository. Follow these steps when creating a new model under `label_studio_ml/examples/<model>`.

## 1. Folder Layout

Each example should contain the following files:

- **README.md** – overview of the model, instructions for running the backend, and description of the labeling configuration. Include quick-start commands and environment variables.
- **model.py** – implementation of `LabelStudioMLBase` with `predict()` and `fit()` methods. Keep functions short and well commented. Reuse helper methods when possible.
- **_wsgi.py** – minimal entry point exposing the `app` for gunicorn. Import the model and define `app` via `make_wsgi_app()`.
- **Dockerfile** – builds an image with only the dependencies required to run the model. Install packages from `requirements.txt`.
- **docker-compose.yml** – example service definition for running the backend locally. Expose `9090` by default.
- **requirements.txt** – pinned dependencies for the model. Optional files `requirements-base.txt` and `requirements-test.txt` may list shared and test deps.
- **tests/** – pytest suite. Provide at least one test that runs `fit()` on labeled tasks and verifies `predict()` returns expected results. Use small fixtures under `tests/` to avoid relying on network access.

## 2. Implementation Tips

- Use environment variables like `LABEL_STUDIO_HOST`, `LABEL_STUDIO_API_KEY`, and `MODEL_DIR` to make the backend configurable.
- Parse the labeling configuration with `self.label_interface` to get tag names, label values and data fields. This ensures the backend works with custom configs.
- Save trained artifacts inside `MODEL_DIR`. Use a stable file name such as `model.pkl` or `model.keras`.
- When training, gather all labeled tasks via the Label Studio SDK and convert each annotation to training samples. Keep network requests minimal and log useful information.
- When predicting, load data referenced in the task (e.g., download the CSV) and return results in Label Studio JSON format.
- Handle missing data gracefully and skip tasks without required inputs.
- Keep the code style consistent with `black` and `flake8` where applicable.

## 3. Documentation

- Reference the main repository README to help users understand how to install and run the ML backend.
- Include labeling configuration examples in the example README so users can quickly reproduce training and inference.
- Provide troubleshooting tips or links to Label Studio documentation such as [Writing your own ML backend](mdc:https:/labelstud.io/guide/ml_create).

## 3.1. Security Best Practices

When implementing ML backends, follow these security guidelines:

- **Model Serialization**: Use secure serialization methods (e.g., PyTorch `state_dict` with `weights_only=True` for PyTorch 2.6+)
- **Input Validation**: Validate all user inputs, file formats, and data types before processing
- **Environment Variables**: Never hardcode sensitive information like API keys; use environment variables
- **File Access**: Restrict file system access to designated directories (`MODEL_DIR`, temp directories)
- **Dependencies**: Pin dependency versions and regularly update for security patches

Example secure model loading:
```python
# Secure PyTorch model loading (PyTorch 2.6+)
try:
    state_dict = torch.load(model_path, weights_only=True)
    model.load_state_dict(state_dict)
except Exception as e:
    logger.error(f"Failed to load model securely: {e}")
    # Fallback or error handling
```

## 3.2. Error Handling and Logging

Implement comprehensive error handling and logging:

- **Structured Logging**: Use consistent log levels and structured messages
- **Graceful Degradation**: Handle missing data, corrupted files, and network issues
- **User-Friendly Errors**: Return meaningful error messages to Label Studio
- **Debug Information**: Log sufficient detail for troubleshooting without exposing sensitive data

Example logging pattern:
```python
import logging
logger = logging.getLogger(__name__)

def predict(self, tasks):
    logger.info(f"Starting prediction for {len(tasks)} tasks")
    try:
        # Prediction logic
        logger.debug(f"Processed task {task_id} successfully")
    except Exception as e:
        logger.error(f"Prediction failed for task {task_id}: {e}")
        return {}  # Return empty result gracefully
```

## 3.3. Performance and Scalability

Consider performance implications for production deployment:

- **Memory Management**: Use generators for large datasets, clear unused variables, handle memory leaks
- **Batch Processing**: Process multiple tasks together when possible to improve throughput
- **Model Loading**: Cache models in memory to avoid repeated loading from disk
- **Resource Monitoring**: Log memory and CPU usage for monitoring and optimization
- **Async Operations**: Use async/await for I/O operations when appropriate

Example efficient model caching:
```python
_model_cache = {}

def get_model(self, model_path):
    if model_path not in _model_cache:
        logger.info(f"Loading model from {model_path}")
        _model_cache[model_path] = self._load_model(model_path)
    return _model_cache[model_path]
```

## 3.4. Model Versioning and Compatibility

Implement proper model versioning for production systems:

- **Version Tracking**: Include model version in predictions and logs
- **Rollback Support**: Maintain ability to revert to previous model versions

Example versioning pattern:
```python
def setup(self):
    self.set("model_version", f"{self.__class__.__name__}-v1.2.3")
    
def predict(self, tasks):
    return ModelResponse(
        predictions=predictions,
        model_version=self.get("model_version")
    )
```

## 3.5. CI/CD Integration

Design backends for automated testing and deployment:

- **Containerization**: Ensure Docker containers build consistently across environments
- **Test Automation**: Include comprehensive test suites that run in CI pipelines
- **Health Checks**: Implement `/health` endpoints for deployment monitoring
- **Configuration Management**: Use environment variables for all configuration
- **Dependency Management**: Pin all dependencies with specific versions

Example health check endpoint:
```python
@app.route('/health')
def health():
    return {"status": "healthy", "model_loaded": _model is not None}
```

## 3.6. Data Handling Patterns

Implement robust data processing for different scenarios:

- **File Format Support**: Handle multiple data formats (CSV, JSON, images, audio) with proper validation
- **Data Preprocessing**: Implement consistent preprocessing pipelines for training and prediction
- **Type Safety**: Use proper type conversion and validation for different data types
- **Streaming Data**: Support large files that don't fit in memory using streaming approaches
- **Data Caching**: Cache preprocessed data when appropriate to improve performance
- **LabelStudioMLBackend::preload_task_data(path, task)**: Download all URLs from a task and stores them locally. It uses get_local_path() `from label_studio_sdk._extensions.label_studio_tools.core.utils.io import get_local_path` and requires LABEL_STUDIO_API_KEY and LABEL_STUDIO_URL to be able to download files through Label Studio instance.

Example robust data loading:
```python
def _read_data(self, task, path):
    """Load data with format detection and error handling."""
    try:
        if path.endswith('.csv'):
            csv_str = self.preload_task_data(task, value=path)
            return pd.read_csv(io.StringIO(csv_str))
        elif path.endswith('.json'):
            json_str = self.preload_task_data(task, value=path)
            return json.loads(json_str)
        else:
            raise ValueError(f"Unsupported file format: {path}")
    except Exception as e:
        logger.error(f"Failed to load data from {path}: {e}")
        return None
```

Common data validation pattern:
```python
def _validate_data(self, df, required_columns):
    """Validate DataFrame has required structure."""
    if df is None or df.empty:
        return False
    
    missing_cols = set(required_columns) - set(df.columns)
    if missing_cols:
        logger.error(f"Missing required columns: {missing_cols}")
        return False
    
    return True
```

## 4. Testing

- Tests should be runnable with `pytest` directly from the repository root or inside the example's Docker container.
- Mock Label Studio API interactions whenever possible to avoid requiring a running server during tests.
- Aim for good coverage of `fit()` and `predict()` logic to catch regressions.

### 4.1. Running Tests in Docker Containers

For ML backends that require specific dependencies or environments, Docker containers provide consistent testing environments. Here's the recommended workflow:

#### Setup and Build
```bash
# Navigate to your example directory
cd label_studio_ml/examples/<your_example>

# Build the Docker container (without --no-cache for faster builds)
docker compose -f docker-compose.yml build

# Start the container in background
docker compose -f docker-compose.yml up -d
```

#### Install Test Dependencies
Most containers won't have pytest installed by default. Install it:
```bash
# Install pytest and coverage tools
docker compose -f docker-compose.yml exec -T <service_name> pip install pytest pytest-cov
```

#### Run Tests
```bash
# Run all tests with verbose output and coverage
docker compose -f docker-compose.yml exec -T <service_name> pytest -vvv --cov --cov-report=xml:/tmp/coverage.xml

# Run specific test files
docker compose -f docker-compose.yml exec -T <service_name> pytest -vvv tests/test_model.py

# Run specific test methods
docker compose -f docker-compose.yml exec -T <service_name> pytest -vvv tests/test_model.py::TestClass::test_method

# Collect available tests without running them
docker compose -f docker-compose.yml exec -T <service_name> pytest --collect-only tests/
```

#### Example: TimeSeries Segmenter
```bash
# Complete testing workflow for timeseries_segmenter
cd label_studio_ml/examples/timeseries_segmenter

# Build and start container
docker compose -f docker-compose.yml build
docker compose -f docker-compose.yml up -d

# Install test dependencies
docker compose -f docker-compose.yml exec -T timeseries_segmenter pip install pytest pytest-cov

# Run full test suite
docker compose -f docker-compose.yml exec -T timeseries_segmenter pytest -vvv --cov --cov-report=xml:/tmp/coverage.xml tests/test_segmenter.py

# Cleanup
docker compose -f docker-compose.yml down
```

#### Troubleshooting Docker Tests

**Issue: Test files not found or outdated**
- Solution: Rebuild the container if test files were modified after the image was built
- Use `docker compose build` (without `--no-cache` unless absolutely necessary)

**Issue: Import errors in container**
- Solution: Ensure all dependencies are in `requirements.txt` and properly installed
- Check that the test file has correct import paths (relative vs absolute)

**Issue: Environment variables not set**
- Solution: Use `patch.dict(os.environ, {...})` in tests or set them in docker-compose.yml
- Override instance attributes directly for test configurations

**Issue: Mock function signature mismatches**
- Solution: Check the actual method signatures and use `side_effect` with lambda functions when needed
- Ensure mock functions match the expected parameter names and types

#### Best Practices for Docker Testing

1. **Consistent Environment**: Use the same base image for development and testing
2. **Fast Iteration**: Avoid `--no-cache` unless dependencies changed
3. **Test Isolation**: Use temporary directories and cleanup fixtures
4. **Comprehensive Logging**: Enable verbose pytest output (`-vvv`) for debugging
5. **Coverage Reports**: Generate coverage reports to ensure thorough testing
6. **Service Names**: Use descriptive service names in docker-compose.yml for clarity

#### Test Documentation in Code

Each test method should include comprehensive docstrings explaining:
- What functionality is being tested
- Expected inputs and outputs
- Critical validations being performed
- Edge cases being handled

Example:
```python
def test_model_training_workflow(self):
    """Test complete end-to-end machine learning pipeline.
    
    This test validates:
    - Full training workflow with real data
    - Training metrics generation (accuracy, F1-score, loss)
    - Model convergence and learning validation
    - Prediction generation on trained model
    
    Critical validation: The complete ML pipeline works from training 
    to prediction, producing valid Label Studio annotations.
    """
```

## 5. Examples

- You can use as an implementation example `label_studio_ml/examples/yolo/`. It's well written and can be a model to follow.

Following these conventions helps maintain consistency across examples and makes it easier for contributors and automation tools to understand each backend.