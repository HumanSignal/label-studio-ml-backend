version: "3.8"

services:
  server:
    container_name: server
    image: humansignal/llm-interactive:v0
    build: .
    environment:
      - MODEL_DIR=/data/models
      # Specify openai model provider: "openai" or "azure"
      - OPENAI_PROVIDER=openai
      # Specify API key for openai or azure
      - OPENAI_API_KEY=
      # Specify model name for openai or azure (by default it uses "gpt-3.5-turbo-instruct")
      - OPENAI_MODEL=
      # Internal prompt template for the model is:
      # **Source Text**:\n\n"{text}"\n\n**Task Directive**:\n\n"{prompt}"
      # if you want to specify task data keys in the prompt (i.e. input <TextArea name="$PROMPT_PREFIX..."/>, set this to 0
      - USE_INTERNAL_PROMPT_TEMPLATE=1
      # Prompt prefix for the TextArea component in the frontend to be used for the user input
      - PROMPT_PREFIX=prompt
      # Log level for the server
      - LOG_LEVEL=DEBUG
      # Number of responses to generate for each request
      - NUM_RESPONSES=1
      # Temperature for the model
      - TEMPERATURE=0.7
      # Azure resourse endpoint (in case OPENAI_PROVIDER=azure)
      - AZURE_RESOURCE_ENDPOINT=
      # Azure deployment name (in case OPENAI_PROVIDER=azure)
      - AZURE_DEPLOYMENT_NAME=
      # Azure API version (in case OPENAI_PROVIDER=azure)
      - AZURE_API_VERSION=2023-05-15
    ports:
      - 9090:9090
    volumes:
      - "./data/server:/data"