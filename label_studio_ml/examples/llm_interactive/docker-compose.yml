version: "3.8"

services:
  llm_interactive:
    container_name: llm_interactive
    image: heartexlabs/label-studio-ml-backend:llm-master
    init: true
    build:
      context: .
      args:
        TEST_ENV: ${TEST_ENV}
    environment:
      - MODEL_DIR=/data/models
      # Specify openai model provider: "openai", "azure", or "ollama"
      - OPENAI_PROVIDER=openai
      # Specify API key for openai or azure
      - OPENAI_API_KEY=
      # Specify model name for openai or azure (by default it uses "gpt-3.5-turbo")
      - OPENAI_MODEL=gpt-3.5-turbo
      # Internal prompt template for the model is:
      # **Source Text**:\n\n"{text}"\n\n**Task Directive**:\n\n"{prompt}"
      # if you want to specify task data keys in the prompt (i.e. input <TextArea name="$PROMPT_PREFIX..."/>, set this to 0
      - USE_INTERNAL_PROMPT_TEMPLATE=1
      # You can define the default prompt to be used before the user input
      # Can be the path to the file with the prompt or the prompt itself
      # ! Note that USE_INTERNAL_PROMPT_TEMPLATE should be set to 0 in this case
      - DEFAULT_PROMPT=
      # Prompt prefix for the TextArea component in the frontend to be used for the user input
      - PROMPT_PREFIX=prompt
      # Log level for the server
      - LOG_LEVEL=DEBUG
      # Number of responses to generate for each request
      - NUM_RESPONSES=1
      # Temperature for the model
      - TEMPERATURE=0.7
      # Azure resourse endpoint (in case OPENAI_PROVIDER=azure)
      - AZURE_RESOURCE_ENDPOINT=
      # Azure deployment name (in case OPENAI_PROVIDER=azure)
      - AZURE_DEPLOYMENT_NAME=
      # Azure API version (in case OPENAI_PROVIDER=azure)
      - AZURE_API_VERSION=2023-05-15
      # Ollama Endpoint (in case OPENAI_PROVIDER=ollama, OPENAI_MODEL=<your_ollama_model>)
      # If running Ollama locally OLLAMA_ENDPOINT=http://host.docker.internal:11434/v1/
      - OLLAMA_ENDPOINT=
      # specify these parameters if you want to use basic auth for the model server
      - BASIC_AUTH_USER=
      - BASIC_AUTH_PASS=
    extra_hosts:
      - "host.docker.internal:host-gateway"  # for macos and unix
    ports:
      - 9090:9090
    volumes:
      - "./data/server:/data"