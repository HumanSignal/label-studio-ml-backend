version: "3.11"

services:
  grounding_sam:
    container_name: grounding_sam
    image: heartexlabs/label-studio-ml-backend:grounding_sam-master
    init: true

    # ← lets "localhost:8080" inside the container actually hit your host LS
    network_mode: host

    build:
      context: .
      dockerfile: Dockerfile
      args:
        TEST_ENV: ${TEST_ENV:-}

    environment:
      MODEL_DIR: /data/models
      WORKERS: "2"
      THREADS: "4"
      LOG_LEVEL: "DEBUG"

      # Add these variables if you want to access the images stored in Label Studio
      LABEL_STUDIO_HOST: "http://172.17.0.1:8080"
      LABEL_STUDIO_ACCESS_TOKEN: "your_label_studio_access_token_here"

      # use these if you want to use segment anything instead of bounding box predictions from input text prompts
      USE_SAM: "true"  # if you want to automatically generate segment anything model predictions
      USE_MOBILE_SAM: "false" # whether you want to use a more efficient, yet a bit less accurate, version of the segment anything model
      BOX_THRESHOLD: "0.30"
      TEXT_THRESHOLD: "0.25"

# # Uncomment the following lines if you want to use GPU
#      - NVIDIA_VISIBLE_DEVICES=all
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: 1
#              capabilities: [gpu]
    ports:
      - "9090:9090"

    command:
      - sh
      - -c
      - |
        pip install "numpy<2"  && /app/start.sh

    # give plenty of RAM if you’re CPU‐only
    deploy:
      resources:
        reservations:
          memory: 16G

    volumes:
      - ./data/ml-backend:/data # ML models & cache
      # ← mount LS’s upload folder so dino.py can open “/data/upload/1/…jpg” directly
      - ~/.local/share/label-studio/media/upload:/data/upload:ro
      - ./prompt.txt:/app/prompt.txt:ro # fallback prompt and custom model logic
      - ./dino.py:/app/dino.py:ro